# %% Locate root dirimport osROOT_DIR = os.path.abspath(".")while not ".prj_root" in os.listdir(ROOT_DIR):    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR, ".."))    # %% Headerimport pickleimport numpy as npimport pandas as pdimport rdflib as rdfimport networkx as nximport matplotlib.pyplot as pltimport rdflib.extras.external_graph_libs as rdfextrafrom collections import defaultdict, Counter# %% HeaderINPUT_FILES = {        "tmQM-RDF-1Ksel": os.path.join(ROOT_DIR, "data", "derivative", "tmQM-RDF-1Ksel", "data", "%s", "1k_selection.csv"),        "processed_patterns": os.path.join(ROOT_DIR, "computational", "pattern_clustering", "results", "%s")    }OUTPUT_FILES = {        "intermediate": os.path.join(ROOT_DIR, "computational", "pattern_clustering", "intermediate", "%s")    }# tmQM_RDF_selection = "./../../../../../data/extract_selection_from_tmQM-RDF/1k_selection.csv"# processed_data_directory = os.path.join("..", "..", "..", "processed_pattern_data")PATTERN_DATASET_KEYWORD = "lateTM"DATASET_SHORT_ID = "latmod" # Ligand ATtachment MODes# Range of sizes of interes (extremes included)SIZE_RANGE = [10, 12]FEATURE_TYPE = "semantic" # One of "proxy" or "semantic"# %% Utility functionsdef pattern_to_document(patt):    """    Computes for each pattern the list of the RDFS classes found within (with occurrences).        Arguments:        - patt: a rdflib.Graph object representing the pattern            Returns:        - a collections.Counter objects that counts the number of occurrences for each label found in patt    """    g = rdfextra.rdflib_to_networkx_digraph(            patt,            False,            lambda s, p, o: {"label": str(p)} # constructs the edge attribute 'label' by converting the predicate into the label        )        node_labels = {}    edge_remove = []        mc_classified = False    classified_ligands = []    classified_atoms = []        for e in g.edges(data = "label"):        predicate = e[2].split("/")[-1]                if predicate == "hasMetalCentre":            # Domain: TransitionMetalComplex            # Range: MetalCentre (unless a more specific class is available)                        node_labels[e[0]] = "resource://integreat/p5/complex/TMC/TransitionMetalComplex"                        if not mc_classified:                node_labels[e[1]] = "resource://integreat/p5/ligand/centre/MetalCentre"                    elif predicate == "hasLigand":            # Domain: TransitionMetalComplex            # Range: Ligand (unless a more specific class is available)                        node_labels[e[0]] = "resource://integreat/p5/complex/TMC/TransitionMetalComplex"                        if e[1] not in classified_ligands:                node_labels[e[1]] = "resource://integreat/p5/ligand/ligand/Ligand"                        elif predicate == "isMetalCentre":            # Domain: MetalCentre            # Range: MetalCentreClass                        if not str(e[1]).split("/")[-2] == "variable":                mc_classified = True                                node_labels[e[0]] = str(e[1])                    edge_remove += [e]                 elif predicate == "bLc":            # Domain: MetalCentre            # Range: LigandBond                        if not mc_classified:                node_labels[e[1]] = "resource://integreat/p5/ligand/centre/MetalCentre"                        node_labels[e[1]] = "resource://integreat/p5/ligand/bond/LigandBond"                        elif predicate == "bLl":            # Domain: Ligand (unless a more specific class is available)            # Range: LigandBond                        if e[0] not in classified_ligands:                node_labels[e[0]] = "resource://integreat/p5/ligand/ligand/Ligand"                        node_labels[e[1]] = "resource://integreat/p5/ligand/bond/LigandBond"                    elif predicate == "isLigand":            # Domain: Ligand            # Range: LigandClass                        classified_ligands += [e[0]]                        node_labels[e[0]] = str(e[1])                        edge_remove += [e]                    elif predicate == "hasBindingAtom":            # Domain: LigandBond            # Range: Atom (unless a more specific class is available)                        node_labels[e[0]] = "resource://integreat/p5/ligand/bond/LigandBond"                        if e[1] not in classified_atoms:                node_labels[e[1]] = "resource://integreat/p5/atomic/atom/Atom"                        elif predicate == "isAtom":            # Domain: Atom            # Range: Element (unless a more specific class is available)                        classified_atoms += [e[0]]                        node_labels[e[0]] = str(e[1])                        edge_remove += [e]        # Add node labels    g.update(nodes = [(v, {"label": l}) for v, l in node_labels.items()])        # Remove edges of the form is...    g.remove_edges_from(edge_remove)        # Remove isolated nodes created by edge removal    g.remove_nodes_from(list(nx.isolates(g)))        # Convert to document        doc = []        doc += [v[-1] for v in g.nodes(data = "label")]    doc += [e[-1] for e in g.edges(data = "label")]        return Counter(doc)# %% Main functionsdef pattern_dataset_to_docs(patterns_directory, pat_sizes, pat_ids):    """    This function converts a pattern dataset into the equivalent 'documents' dataset (see pattern_to_document)        Arguments:        - patterns_directory: the path to directory where patterns are stored (by size) as RDF graphs        - pat_sizes: a dictionary that associates to each size of interest the number of patterns of that size        - pat_ids: a list of pattern ids arranged by size, i.e. [...pat of size i - 1...pat of size i...pat of size i + 1...]            Returns:        - a dictionary that associates to each size of interest a list of patterns compressed into their 'document' representation        - the set of all the labels encountered during processing    """    sizes_of_interest = pat_sizes.keys()        # Networkx graph computation    buffer1 = 0    pat_docs = {i: [] for i in sizes_of_interest}    label_set = []    for i in sizes_of_interest:        for p_id in pat_ids[buffer1:(buffer1 + pat_sizes[i])]:            temp1 = rdf.Graph()            temp1.parse(os.path.abspath(os.path.join(patterns_directory, f"{i}", f"{p_id}.nt")))                        doc = pattern_to_document(temp1)            pat_docs[i] += [doc]            label_set += list(doc.keys())                    buffer1 += pat_sizes[i]            return pat_docs, list(set(label_set))def compute_tfidf_weights(pat_docs, label_set):        term_freq = dict()        # Term frequency    buffer = 0    for _, pat_batch in pat_docs.items():        for i, pat in enumerate(pat_batch):            total = sum(c for c in pat.values())            term_freq[buffer + i] = [pat[l]/total if l in pat else 0 for l in label_set]                    buffer += len(pat_batch)                    term_freq = pd.DataFrame(term_freq, index = label_set)        # Inverse document frequency    count = defaultdict(int)    for size, pat_batch in pat_docs.items():        for pat in pat_batch:            for l in set(pat.keys()):                count[l] += 1        n_pat = sum([len(pat_batch) for pat_batch in pat_docs.values()])    inv_doc_freq = np.diag([1 + np.log(n_pat/(1 + count[l])) for l in label_set])        return inv_doc_freq @ np.array(term_freq), term_freq, inv_doc_freq# %% __main__if __name__ == "__main__":        # Compute dataset tag (for dataset identification)    if SIZE_RANGE[1] > SIZE_RANGE[0]:        size_tag = f"s_{SIZE_RANGE[0]}_{SIZE_RANGE[1]}"    else:        size_tag = f"s_{SIZE_RANGE[0]}"        dataset_tag = f"{PATTERN_DATASET_KEYWORD}-{DATASET_SHORT_ID}-{size_tag}"        # Resolve working directories    target_directory = os.path.join(INPUT_FILES["processed_patterns"] % dataset_tag)        results_directory = os.path.join(target_directory)        local_output_directory = os.path.join(OUTPUT_FILES["intermediate"] % dataset_tag, "cossim", f"features_{FEATURE_TYPE}")        patterns_directory = os.path.join(target_directory, "patterns", "rdf")        if not os.path.exists(local_output_directory):        os.makedirs(local_output_directory)        # Pattern ids extraction (read column names from matches.csv, notice that first entry is the index column)    # This guarantees that patterns appear in the same order both in the similarity matrix and in the match-feature matrix    with open(os.path.join(results_directory, "matches.csv"), "r") as f:        pat_ids = eval(f.read().split("\n")[0])[1:]          # Pattern sizes extraction    sizes_of_interest = range(SIZE_RANGE[0], SIZE_RANGE[1] + 1)    pat_sizes = {i: len(os.listdir(os.path.join(patterns_directory, f"{i}"))) for i in sizes_of_interest}        # Parse dataset as documents    pat_docs, label_set = pattern_dataset_to_docs(patterns_directory, pat_sizes, pat_ids)        if FEATURE_TYPE == "semantic":        X, tf, idf = compute_tfidf_weights(pat_docs, label_set)                with open(os.path.join(local_output_directory, "dist_weights.pkl"), "wb") as f:            pickle.dump((X, tf, idf), f)    elif FEATURE_TYPE == "proxy":        df = pd.read_csv(os.path.join(results_directory, "matches.csv"), index_col = 0)                selection = pd.read_csv(INPUT_FILES["tmQM-RDF-1Ksel"] % PATTERN_DATASET_KEYWORD, index_col = 0)        train = selection.loc[selection["partition"] == "train", "TMC"]                X = np.sign(np.array(df.loc[train, :]))    else:        raise Exception(f"Feature type {FEATURE_TYPE} not recognised!")            # Compute cosine similarity    XtX = X.T @ X        D = np.sqrt(np.matrix(np.diag(XtX.diagonal())).I)        sims = np.clip(np.array(D @ XtX @ D), 0, 1)        with open(os.path.join(local_output_directory, "dist_matrix.pkl"), "wb") as f:        pickle.dump(1 - sims, f)        print("Similarity computed")        # plt.imshow(sims)        fig, ax = plt.subplots()    ax.hist(sims[np.triu_indices(sims.shape[0], 1)], bins = 100)    ax.set_xlim(0,1)    ax.set_title(f"s_cos ({FEATURE_TYPE})")